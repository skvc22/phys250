{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Lab 03 : Numerical Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "The main objectives of this prelab are as follows.\n",
    "\n",
    "- **Avoid writing loops!** Become more familiar with using arrays and the functions that act on them.\n",
    "- Start using markup, in particular LaTeX, in notebooks for good and elegant documentation.\n",
    "- Learn and understand the use of Richardson extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "As always you should add initialization to the top of your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30df9b3aaf57a082aa3ddc5798a31a08",
     "grade": true,
     "grade_id": "cell-95c8993f273139f2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('xtick', direction='in', top=True)\n",
    "mpl.rc('ytick', direction='in', right=True)\n",
    "mpl.rc('xtick.minor', visible=True)\n",
    "mpl.rc('ytick.minor', visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Loops\n",
    "\n",
    "We have been warned many times to avoid writing loops in Python, as they can be slow and prone to coding errors.\n",
    "We have seen a few features of NumPy that allow us to avoid writing loops: functions work on arrays of values and array slicing allows us to view an array in many ways without having to create a new array.\n",
    "We will continue to learn about these features along with a few other ones.\n",
    "Often we write loops to perform mathematical operations. In languages that include vector processing capabilities, provided in Python through the NumPy module, there is almost always a function that can be used to perform the calculation without the need to write such a loop.\n",
    "The price we pay for this is that we must be able to store all of the information needed for the calculation in arrays. In other words, the trade off is between memory usage and computation speed: with little memory usage we can write a loop which will be slow, with a lot of memory usage we can perform the calculation \"all at once\" using a built-in function which is often significantly faster than the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sums\n",
    "\n",
    "As a simple example of this consider a sum of integers, this has a known analytic result:\n",
    "$$ \\sum_{j=1}^N j = \\frac{N(N+1)}{2}. $$\n",
    "We can easily evaluate this sum by constructing an array holding all the integers from $1$ to $N$ (including $N$) and using the `sum` function from NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the use of the NumPy `sum` function by evaluating this sum for various values of $N$ and verifying it agrees with the analytic result. Show the result for one case (for example $N=12$).\n",
    "(*Note*: Python also has a `sum` function which is similar, but not the same as the one from NumPy. For a NumPy array we want to use the `sum` function from NumPy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7ad52f3fcf4b65974d700e95e8cfc73",
     "grade": true,
     "grade_id": "cell-35b325e651602e37",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy sum:  78 \n",
      "Analytic result:  78\n"
     ]
    }
   ],
   "source": [
    "N=12\n",
    "arr = np.arange(1,N+1, 1)\n",
    "print(\"NumPy sum: \", np.sum(arr), \"\\nAnalytic result: \", (N*(N+1))//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that `sum` returns an integer if the array you are summing over is an integer type, whereas in Python 3 division (even of integers) returns a floating point number. Recall from the previous lab that you can force integer division by using `//` instead of `/`. It sometimes matters that a quantity is an integer instead of a float and sometimes it does not! Here it really does not matter, though by getting the float we can immediately verify that the analytic formula does only produce an integer result (as it must)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above to verify the sum over the square of the integers satisfies\n",
    "\n",
    "$$ \\sum_{j=1}^N j^2 = \\frac{N(N+1)(2N+1)}{6}. $$\n",
    "\n",
    "Include your code and results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff5716d479290a4187ceadb34dfcf54",
     "grade": true,
     "grade_id": "cell-910117bb252da935",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy sum of squares:  650 \n",
      "Analytic result:  650\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy sum of squares: \", np.sum(np.square(arr)), \"\\nAnalytic result: \", (N*(N+1)*((2*N)+1))//6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the expressions from above determine the analytic expectation for\n",
    "\n",
    "$$ \\sum_{j=1}^N (j+1)(2j+1) $$\n",
    "\n",
    "and verify the result numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{j=1}^N (j+1)(2j+1) = \\sum_{j=1}^N j^2+3j+1 = \\sum_{j=1}^N j^2 + \\sum_{j=1}^N 3j + \\sum_{j=1}^N 1 $$\n",
    "\n",
    "$$ \\frac{1}{6}N(N+1)(2N+1) + \\frac{3}{2}N(N+1) + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "381b19c7fdbfd6d19ac3549eebbd3b78",
     "grade": true,
     "grade_id": "cell-1c760270692418c6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy sum of j^2:  650 \n",
      "Analytic result:  650.0\n",
      "\n",
      "NumPy sum of 3j:  234 \n",
      "Analytic result:  234.0\n",
      "\n",
      "Twelve:  12\n",
      "\n",
      "NumPy total:  896\n",
      "Analytic total:  168\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy sum of j^2: \", np.sum(np.square(arr)), \"\\nAnalytic result: \", (1/6)*(N*(N+1)*((2*N)+1)))\n",
    "print()\n",
    "print(\"NumPy sum of 3j: \", np.sum(3*arr), \"\\nAnalytic result: \", (3/2)*(N*(N+1)))\n",
    "print()\n",
    "print(\"Twelve: \", 12)\n",
    "print()\n",
    "print(\"NumPy total: \", np.sum(np.square(arr)) + np.sum(3*arr) + 12)\n",
    "print(\"Analytic total: \", ((1//6)*(N*(N+1)*((2*N)+1))) + ((3//2)*(N*(N+1))) + 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely for fun (i.e., it is not graded) also numerically evaluate the sum\n",
    "$$ \\sum_{j=1}^{9} j^3. $$\n",
    "Be amazed (if you did not already know this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4aa4c4be3986a8e3dccf4b66250ef9db",
     "grade": true,
     "grade_id": "cell-2b2dc8f83e2602d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy sum of j^3:  2025\n"
     ]
    }
   ],
   "source": [
    "newN=9\n",
    "newarr = np.arange(1,newN+1, 1)\n",
    "print(\"NumPy sum of j^3: \", np.sum(np.power(newarr, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Functions\n",
    "\n",
    "There are many, many other functions which can be discovered by looking through the documentation, searching online, *etc*. In general you should *expect* there to be a function that can be used to perform whatever calculation you want, or you should be able to understand why it does not exist. (There are a few cases where loops are required, for example in many iterative calculations.)\n",
    "\n",
    "For Richardson extrapolation as discussed in the example this week we encountered two other ones, `diag` and `diff`. You should look up their documentation and understand what they do!\n",
    "We will have reasons to use them in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Richardson extrapolation\n",
    "\n",
    "Here we will explore how _Richardson extrapolation_ can be used to calculate derivatives of functions in an efficient manner. In the lecture, we derived the forward differencing formula,\n",
    "\n",
    "$$f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} + \\mathcal{O}(h) + \\cdots .$$\n",
    "\n",
    "We will use Richardson extrapolation to calculate higher order terms in this formula. To begin, our first estimate of the derivative is given by\n",
    "\n",
    "$$ F_1(h) = \\frac{f(x_0 + h) - f(x_0)}{h} = a_1 + \\alpha_1 h + \\mathcal{O}(h^2) .$$\n",
    "\n",
    "Richardson extrapolation works with two (small) step sizes, $h$ and a $h/q$ for some $q > 1$. In the lecture we only considered $q=2$, but in principle we can be more general. For this second step size, our estimate of the derivative is:\n",
    "\n",
    "$$ F_1(h/q) = \\frac{f(x_0 + h/q) - f(x_0)}{h/q} = a_1 + \\alpha_1 h/q + \\mathcal{O}((h/q)^2) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside: some basic LaTeX.\n",
    "\n",
    "Notebooks, such as this one, provide both code and documentation. The documentation includes formatting through *Markdown* and equations using LaTeX. We will become more familiar with some aspects of creating documentation throughout the semester.\n",
    "\n",
    "Though it is possible to use unicode in the notebook to include a huge range of symbols, a much better way of formatting equations is to typeset them using LaTeX. If you are unfamiliar with LaTeX you should become familiar with it. It is by far the best system available for typesetting documents, particularly those involving mathematics, and is the standard tool used in communicating results in the sciences (at least in physics). It was written over 40 years ago by Donald Knuth in response to electronic proofs of a book he had written. At the time, publishers were switching from hand typesetting to electronic typesetting of documents. The proofs were so poorly typeset that he wrote his own system. This includes tools for creating fonts and a complete set of fonts with all the needed mathematical symbols (something nonexistent at the time and only recently has it become more common to find fonts with complete sets of mathematical symbols). You should also be familiar with Donald Knuth. He is arguably the greatest computer scientist ever, has/is writing the quintessential books on computer algorithms, *The Art of Computer Programming*, and is a graduate of the Case Institute of Technology. Yes, he wandered these same halls in Rockefeller over 50 years ago.\n",
    "\n",
    "Here we will get a brief introduction to typesetting simple mathematical expressions. These can appear in the documentation in the notebook but also in figures.\n",
    "\n",
    "To typeset a formula we use LaTeX in markdown cells. Many examples of this are included in notebooks. You can look at the unformatted text in any notebook by double clicking on the cell containing the text (which you should now do). To insert an equation, the dollar sign, `$`, is used to denote where the equation begins and ends. In the notebook, you should wrap any latex code in dollar characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Basic Equation\n",
    "\n",
    "To get you warmed up try typing in a simple equation, such as $x + y = 3$ but pick your own for fun, surrounded by `$` signs.\n",
    "\n",
    "*Your Equation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1962d2d8ee5a8115fe1ebdbb127a3a0b",
     "grade": true,
     "grade_id": "cell-b49d005da5e25ed2",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$x^2+7x+12=4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superscripts and Subscripts\n",
    "\n",
    "Not every equation is so simple. To create superscripts and subscripts characters use the caret character `^` and underscore `_`, respectively. These will only raise the characters immediately after them, unless you wrap subsequent text in curly braces, such as `x^{1234}`. To get an idea of how this works, try typing some superscripts or subscripts below. Don't forget to include the `$` signs!\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- `x^1` gives $ x^1 $\n",
    "- `x^12` gives $ x^12 $  (notice what happens if you do not use curly braces)\n",
    "- `x^{12}` gives $ x^{12} $\n",
    "\n",
    "*Your Superscript/Subscript:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b22f1bbe674a89b0e9fd3619388d0ce9",
     "grade": true,
     "grade_id": "cell-1520d088381361b5",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$x_a + x_b^2 = x_{\\text{I'm running out of ideas}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fractions\n",
    "\n",
    "Fractions are typeset using `\\frac{ numerator }{ denominator }`. Try typing a fraction below.\n",
    "\n",
    "###### Fraction Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ef6a98143daa2a2f96e7a3a9393f0df",
     "grade": true,
     "grade_id": "cell-2d5790f8ef2bf465",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$\\frac{22}{7}\\approx\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other things to learn about LaTeX, but this is enough to get us started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Richardson extrapolation\n",
    "\n",
    "We wish to implement Richardson extrapolation for the forward differencing algorithm. To begin, recall from class and the example this week that to find an expression for $a_1$ we need to perform a few steps. One way to perform the calculation is the algebraic manipulations of first multiplying the expression for $F_1(h/q)$ by $q$; then subtracting this result from the equation for $F_1(h)$; and finally solving for $a_1$, or $F_2(h) \\equiv a_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform these steps and provide your answer typeset using LaTeX below. For your result let $q=2$ and leave your answer in terms of $F_1$.\n",
    "\n",
    "*First step of Richardson extrapolation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11481bcdd6f22408ba2b2f1dccc4c5f7",
     "grade": true,
     "grade_id": "cell-36a55b59fee5b5c7",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$ F_1(h)-2F_1(h/2) = \\left( a_1 - 2a_1 \\right) + \\left( \\alpha_1 h - \\alpha_1 h \\right) + \\left( \\mathcal{O}(h^2) - \\mathcal{O}(h^2/2) \\right)$$\n",
    "$$ F_1(h)-2F_1(h/2) = -a_1$$\n",
    "$$ a_1 = 2F_1(h/2) - F_1(h) = F_2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step can be improved by further steps of Richardson extrapolation. Derive a general expression for the next step in Richardson extrapolation, $F_{i+1}(h)$, in terms of the previous step, $F_i$, for forward differencing. Continue to use $q=2$.\n",
    "\n",
    "*General step of Richardson extrapolation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f0db1869dce02249bbe836c05450781",
     "grade": true,
     "grade_id": "cell-9a51fc283f1b4b0b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$ F_{i+1} = \\frac{2^iF_i(\\frac{h}{q}) - F_i(h)}{2^i-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparison of convergence rates:*\n",
    "\n",
    "If you compare your expression to the one found in class and the example notebook for center differencing, it can be seen that the forward differencing formula should converge more slowly. Explain below why you think this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6a992f2cd403a7f8ae37fb757944757",
     "grade": true,
     "grade_id": "cell-978aa5da09ca9037",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The forward differencing formula converges more slowly because its error scales with $h^j$, where initially $j=1$ and is incremented by 1 for every step of Richardson extrapolation. The center differencing formula, on the other hand, scales with $h^{2j}$ where again $j=1$ at first and is incremented by 1 for every step of extrapolation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Richardson extrapolation\n",
    "\n",
    "We will now attempt to translate the above formula to code. The example notebook from this week includes an implementation of this algorithm for center differencing. This code has been copied below for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def richardson_center (f, z, h, nsteps, args=()):\n",
    "    \"\"\"Evaluate the first derivative of a function at z, that is f'(z),\n",
    "    using Richardson extrapolation and center differencing.\n",
    "\n",
    "    Returned is the full table of approximations, Fij for j <= i. The\n",
    "    values of Fij for j > i are set to zero. The final value F[-1,-1]\n",
    "    should be the most accurate estimate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        Vectorized Python function.\n",
    "        This is the function for which we are estimating the derivative.\n",
    "    z : number\n",
    "        Value at which to evaluate the derivative.\n",
    "    h : number\n",
    "        Initial stepsize.\n",
    "    nsteps : integer\n",
    "        Number of steps to perform.\n",
    "    args : tuple, optional\n",
    "        extra arguments to pass to the function, f.\n",
    "    \"\"\"\n",
    "    # Extra check to allow for args=(1) to be handled properly. This is a\n",
    "    # technical detail that you do not need to worry about.\n",
    "    if not isinstance(args, (tuple, list, np.ndarray)):\n",
    "        args = (args,)\n",
    "    # Create a zero filled table for our estimates\n",
    "    F = np.zeros((nsteps, nsteps))\n",
    "    # First column of F is the center differencing estimate.\n",
    "    # We can fill this without a loop!\n",
    "    harr = h / 2.**np.arange(nsteps)\n",
    "    F[:,0] = (f(z+harr, *args) - f(z-harr, *args)) / (2.*harr)\n",
    "    # Now iterate, unfortunately we do need one loop. We could\n",
    "    # get rid of the inner loop but the algorithm is a little easier to\n",
    "    # understand if we do not.\n",
    "    for i in range(1, nsteps):\n",
    "        fact = 0.25\n",
    "        for j in range(1, i+1):\n",
    "            F[i,j] = F[i-1,j-1] - (F[i-1,j-1] - F[i,j-1])/ (1-fact)\n",
    "            fact *= 0.25\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Code for Richardson extrapolation of forward differencing:*\n",
    "\n",
    "Using this as a model, implement an algorithm based on the formula you developed in the previous part. Include the code below. This code will be used on the homework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd931c0282769e2d96e3679d455d751",
     "grade": true,
     "grade_id": "cell-d1d4bcf5afeeba9b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def richardson_forward (f, z, h, nsteps, args=()):\n",
    "    \"\"\"Evaluate the first derivative of a function at z, that is f'(z),\n",
    "    using Richardson extrapolation and center differencing.\n",
    "\n",
    "    Returned is the full table of approximations, Fij for j <= i. The\n",
    "    values of Fij for j > i are set to zero. The final value F[-1,-1]\n",
    "    should be the most accurate estimate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        Vectorized Python function.\n",
    "        This is the function for which we are estimating the derivative.\n",
    "    z : number\n",
    "        Value at which to evaluate the derivative.\n",
    "    h : number\n",
    "        Initial stepsize.\n",
    "    nsteps : integer\n",
    "        Number of steps to perform.\n",
    "    args : tuple, optional\n",
    "        extra arguments to pass to the function, f.\n",
    "    \"\"\"\n",
    "    # Extra check to allow for args=(1) to be handled properly. This is a\n",
    "    # technical detail that you do not need to worry about.\n",
    "    if not isinstance(args, (tuple, list, np.ndarray)):\n",
    "        args = (args,)\n",
    "    # Create a zero filled table for our estimates\n",
    "    F = np.zeros((nsteps, nsteps))\n",
    "    # First column of F is the center differencing estimate.\n",
    "    # We can fill this without a loop!\n",
    "    harr = h / 2.**np.arange(nsteps)\n",
    "    F[:,0] = (f(z+harr, *args) - f(z, *args)) / harr\n",
    "    # Now iterate, unfortunately we do need one loop. We could\n",
    "    # get rid of the inner loop but the algorithm is a little easier to\n",
    "    # understand if we do not.\n",
    "    for i in range(1, nsteps):\n",
    "        fact = 0.5\n",
    "        for j in range(1, i+1):\n",
    "            F[i,j] = F[i-1,j-1] - (F[i-1,j-1] - F[i,j-1])/ (1-fact)\n",
    "            fact *= 0.5\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this function to find a derivative of a function. As an example, we can try this for $\\cos(x)$ where $x=1.7$. We know the derivative of $\\cos(x)$ to be $-\\sin(x)$, so we expect the derivative to be $-\\sin(1.7)$.\n",
    "\n",
    "Use `richardson_forward` and `richardson_center` to calculate the derivative of $\\cos(x)$ at $x=1.7$, using a stepsize $h=0.1$ and 4 steps. Compare these to $-\\sin(1.7)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92fd725a8831bfc7a21eaecfbdb89e67",
     "grade": true,
     "grade_id": "cell-a4966cc22994a30e",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward differencing:\n",
      "Value:  -0.9916647976000266\n",
      "Difference from -sin(x):  1.2852441999555708e-08\n",
      "\n",
      "Center differencing:\n",
      "Value:  -0.9916648104524663\n",
      "Difference from -sin(x):  2.220446049250313e-15\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward differencing:\")\n",
    "print(\"Value: \", richardson_forward(np.cos, 1.7, .1, 4)[-1][-1])\n",
    "print(\"Difference from -sin(x): \", np.abs(-np.sin(1.7) - richardson_forward(np.cos, 1.7, .1, 4)[-1][-1]))\n",
    "print(\"\\nCenter differencing:\")\n",
    "print(\"Value: \", richardson_center(np.cos, 1.7, .1, 4)[-1][-1])\n",
    "print(\"Difference from -sin(x): \", np.abs(-np.sin(1.7) - richardson_center(np.cos, 1.7, .1, 4)[-1][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in the PreLab\n",
    "\n",
    "All prelabs will be handled as was done for PreLab01. See that file for details. It will be assumed from now on that you have read and understood the procedure and what it means when you submit a prelab."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "authors": [
   {
    "name": "Craig J Copi",
    "semester": "Spring 2025"
   }
  ],
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
